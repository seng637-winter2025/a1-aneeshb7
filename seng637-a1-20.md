>   **SENG 637 - Software Testing, Reliability, and Quality**

**Lab. Report \#1 – Introduction to Testing and Defect Tracking**

| Group: 20      |
|-----------------|
| Aneesh Bulusu                |   
| Long Nguyen              |   
| Strahinja Radakovic               |   


**Table of Contents**

(When you finish writing, update the following list using right click, then
“Update Field”)

[1 Introduction	1](#_Toc439194677)

[2 High-level description of the exploratory testing plan	1](#_Toc439194678)

[3 Comparison of exploratory and manual functional testing	1](#_Toc439194679)

[4 Notes and discussion of the peer reviews of defect reports	1](#_Toc439194680)

[5 How the pair testing was managed and team work/effort was
divided	1](#_Toc439194681)

[6 Difficulties encountered, challenges overcome, and lessons
learned	1](#_Toc439194682)

[7 Comments/feedback on the lab and lab document itself	1](#_Toc439194683)

# Introduction

Software testing is the process of assessing the quality and reliability of a software to ensure a product is functional and meets specifications and requirements, and as such, it is the unsung hero of software development.
In this lab, we are presented with two versions of a program, which simulates an automated teller machine (ATM). After familiarizing ourselves with the application, we will perform exploratory testing on the initial version of the application (ATM System - Lab 1 Version 1.0), following an exploratory test plan.
We will then perform scripted testing on the initial version of the application, allowing us to compare the results of the two tests. Lastly we will perform regression testing on the updated application (ATM System - Lab 1 Version 1.1) to evaluate the update.


The main difference between exploratory and manual functional testing is the level of pre-planning. Manual functional testing requires a list of predefines test cases which are then assessed manually by a tester, whereas exploratory testing relies on a tester to find error in a more improvisatory way.

# High-level description of the exploratory testing plan

This document outlines the test plan for the Automated Teller Machine (ATM) software. The objective of the testing process is to validate the correctness and reliability of the ATM software based on the provided requirements. The testing approach will focus on covering most functionalities broadly by following common customer use cases.

## 2. Test Types
### 2.1 Functional Testing
- **Card Insertion & Authentication**: Validate ATM card reading and PIN verification.
- **Transaction Processing**: Verify withdrawal, deposit, transfer, and balance inquiry functionalities, prioritizing the most common transactions.
- **Receipt Printing**: Ensure correct receipt generation after transactions.
- **Transaction Cancellation**: Verify that transactions can be aborted at any stage.
- **Operator Functions**: Validate startup, shutdown, cash verification.

### 2.2 Security Testing
- **PIN Security**: Ensure PINs are not stored or logged.
- **Card Retention**: Verify the system retains the card after three incorrect PIN attempts.
- **Data Encryption**: Confirm secure communication with the bank.
- **Fraud Scenarios**: Test for unauthorized access attempts and system responses.

### 2.3 Usability Testing
- **User Interface**: Verify intuitive navigation and clear messaging.
- **Error Handling**: Ensure informative error messages for failed transactions.
- **Accessibility**: Confirm ATM usability for people with disabilities.

## 3. Scope of Testing
The scope of non-scripted manual testing covers all customer and operator functionalities, with emphasis on security and transaction-related operations. The focus will be on:
- Common transactions: Withdrawals and balance inquiries.
- Bank communication: Approval and validation of transactions.

## 4. Test Approach
The testing strategy will involve a combination of broad functional testing and in-depth testing of critical functions:
- **Most functions will be tested lightly**, ensuring all features work as expected.
- **High-priority functions (authentication, transactions, error handling) will be tested extensively**, including both normal and exceptional paths.
- **Test cases will be designed based on real-world usage patterns**, covering frequent and edge-case scenarios.
- **Automated tests will be implemented where feasible** for regression and performance testing.

## 5. Test Logistics

### 5.1 Roles & Responsibilities
- Aneesh Bulusu will drive the non-scripted testing outlined in this plan, while Long Nguyen and Strahinja Radakovic will observe and record.

### 5.2 Test Environment
- Simulated ATM software setup (SUT V 1.0)
- Test cards with valid and invalid PINs.
- Logging and monitoring tools.
- Automated testing framework for regression testing.

### 5.3 Test Data
- Test accounts with different balances and transaction histories.
- Valid and invalid ATM cards as provided in lab instructions.

## 6. Conclusion
This test plan ensures that the ATM software meets functional, security, performance, and usability requirements before deployment. By prioritizing high-risk and frequently used functions while maintaining broad test coverage, this approach ensures a balanced and effective testing process.


# Comparison of exploratory and manual functional testing

Exploratory and manual functional testing both had their benefits and drawbacks. With exploratory testing, one of the drawbacks was finding it difficult to reproduce bugs or issues, as it was an unscripted process where the testers were trying to act as a customer using the system in a production environment. To this end, it made it difficult to keep track of the paths that the testers were going down, but performing peer testing slightly alleviated this issue as the observer was able to help with this process. One of the main benefits was finding bugs "by accident" in a sense. For example, when testing withdrawals and deposits during non-scripted testing, the tester decided to quickly check `Balance inquiry` out of curiosity. It was here that we first observed in SUT version 1.0, the Savings account was not available as an option under `Balance inquiry`. With MFT, the opposite was true. Having test cases organized and strictly defined made it easier to keep track of paths and report bugs that were found in a detailed manner, but the rigidity of the test cases made it easier to miss or ignore bugs related to other functionality, as we were meant to follow specific paths and reset the SUT to an idle and working state before progressing to the next test case. Overall, both testing types were necessary to try and ensure a comprehensive testing process that uncovered as many bugs as possible. Overall, the MFT was perhaps more efficient due to have the steps and rules outlined beforehand, but the exploratory testing was more effective for testing use cases that a customer would run into as that is the mindset we tried to employ while conducting this testing.

# Notes and discussion of the peer reviews of defect reports

After reviewing each others defect reports, we found that one group was forgetting to record the version of the SUT during testing. This lead to a discussion on which stage of testing they were conducting that resolved this. One group also reported a defect with the login and card validation step, which the other group correctly pointed out was due to a typo in the PIN they were entering, and not due to any issues present with the SUT overall. Otherwise, similar bugs and defects were found by both groups and the process was quite productive for streamlining testing and reporting processes for both.

# How the pair testing was managed and team work/effort was divided 

Aneesh Bulusu spear headed the pair testing. The pair testing was conducted over an online live video-chat software (Discord) where Aneesh shared his screen while testing the application, while Long Nguyen and Strahinja Radakovic recorded observed errors. Then Long and Strahinja discussed the errors, making sure to avoid reporting redundancy.
The manual functional testing was lead by Long Nguyen, while Strahinja Radakovic was in charge of regression testing. Discussions between members were commonplace for all three major tests.

# Difficulties encountered, challenges overcome, and lessons learned

While working on the assignment, two main difficulties occurred; learning to use Jira, and the high barrier to entry caused by the assignment length and specifications.

Jira was an unfamiliar platform for our group, and as such, we needed to go out of our way to learn how to use it, which was a considerable time sink.

The assignment length and specifications added more complexity. Firstly, the length of the assignment made it hard to keep track of what and how tasks are to be done. This was further compounded by the formatting requests (ie. the report must be made using markdown), which we aren't used to making reports in.

Besides those issues, the assignment was not very challenging.

# Comments/feedback on the lab and lab document itself

We believe the assignment could be improved by simplifying it, at least when it comes to delivery. The lab itself was not very difficult, but it's size and some unfamiliarity made it more difficult than it needed to be.